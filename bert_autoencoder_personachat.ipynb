{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_autoencoder_personachat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meti-94/TextGeneration/blob/main/bert_autoencoder_personachat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdM9KHPaRNmD"
      },
      "source": [
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar -xf  simple-examples.tgz\n",
        "!mkdir data\n",
        "!mv /content/simple-examples/data/ptb.train.txt data/\n",
        "!mv /content/simple-examples/data/ptb.valid.txt data/\n",
        "!mv /content/simple-examples/data/ptb.test.txt data/\n",
        "!rm -rf ./simple_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B71K5lC_Xnyg"
      },
      "source": [
        "!pip install transformers\n",
        "!wget https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\n",
        "!mkdir models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iivSufj9QhvS",
        "outputId": "b12f7e1e-28fc-43b5-898f-a7cb9188b9ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S5ZszffTUhl"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import EncoderDecoderModel, BertTokenizer, BertModel, BertConfig\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random \n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVUFk1dqSTbG"
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<persona>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>', '<persona>']}\n",
        "\n",
        "def read_data(data_json_path='/content/personachat_self_original.json'):\n",
        "  with open(data_json_path) as json_file:\n",
        "    data_dict = json.load(json_file)\n",
        "  return data_dict\n",
        "\n",
        "def data_to_samples(data_dict, test=False):\n",
        "  samples=[]\n",
        "  for dialogue in (data_dict['train'] if test==False else data_dict['valid']):\n",
        "    original_persona = dialogue['personality']\n",
        "    \n",
        "    for item in dialogue['utterances']:\n",
        "      original_persona = [original_persona[-1]] + original_persona[:-1]\n",
        "      history = item['history']\n",
        "      response = item['candidates'][-1]\n",
        "      samples.append({\n",
        "          'persona':original_persona,\n",
        "          'history':history,\n",
        "          'response':response\n",
        "      })\n",
        "  return samples\n",
        "  \n",
        "def bertified(samples):\n",
        "  bertified_data = []\n",
        "  for item in samples:\n",
        "    persona = ' <persona> '.join(item['persona'])\n",
        "    persona = '<bos> ' + persona\n",
        "    history = ''\n",
        "    speakers = [\" <speaker1> \", \" <speaker2> \"]\n",
        "    speaker = 0\n",
        "    for hst in item['history'][::-1]:\n",
        "      history = speakers[speaker] + hst + history\n",
        "      speaker = 1 - speaker\n",
        "    response = '<speaker2> ' + item['response'] + ' <eos>'\n",
        "    bertified_data.append({\n",
        "          'persona':persona.replace('  ', ' '),\n",
        "          'history':history.replace('  ', ' '),\n",
        "          'input': persona.replace('  ', ' ')+' '+history.replace('  ', ' '), \n",
        "          'response':response.replace('  ', ' ')\n",
        "      })\n",
        "  return bertified_data\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iV8XBcrRTcQ"
      },
      "source": [
        "class PersonaDataset_v1(Dataset):\n",
        "  '''\n",
        "      Convert Data to proper Tensor dataset\n",
        "  '''\n",
        "  def __init__(self, samples):\n",
        "    self.samples = samples\n",
        "    self.n_samples = len(self.samples)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # returns specific item\n",
        "    return self.samples[index] \n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "    # returns dataset length\n",
        "\n",
        "\n",
        "class PTBDataset(Dataset):\n",
        "  '''\n",
        "      Convert Data to proper Tensor dataset\n",
        "  '''\n",
        "  def __init__(self, path):\n",
        "    self.texts = []\n",
        "    with open(path, 'r') as fin:\n",
        "      for line in fin:\n",
        "        self.texts.append(line.strip())\n",
        "    self.n_samples = len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # returns specific item\n",
        "    return self.texts[index] \n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "    # returns dataset length\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj4jILQtXmnt"
      },
      "source": [
        "class TrainingLoop:\n",
        "  '''\n",
        "  Everything related to model training\n",
        "  '''\n",
        "  def __init__( self, model, tokenizer, optimizer, freezeemb=True, \n",
        "                epochs=6, save_path='./models/', **kw):\n",
        "    self.model = model\n",
        "    params = []\n",
        "    for paramname, param in self.model.named_parameters():\n",
        "      if paramname.startswith(\"bert.embeddings.word_embeddings\"):\n",
        "        if not freezeemb:\n",
        "          params.append(param)\n",
        "      else:\n",
        "        params.append(param)\n",
        "    self.optimizer = optimizer(params, **kw)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.epochs = epochs\n",
        "    self.save_path = save_path\n",
        "    self.predicts = None\n",
        "\n",
        "\n",
        "  def train(self, dataloader, eval_dataloader, test_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = self.model.to(device)\n",
        "    for epoch in range(self.epochs):\n",
        "      self.model.train()\n",
        "      losses = []\n",
        "\n",
        "      for _, batch in enumerate(tqdm(dataloader, position=0, leave=True, desc=f\"Train Epoch Number {epoch+1}\")):\n",
        "        self.model.zero_grad()\n",
        "        X = self.tokenizer(batch['input'], add_special_tokens=True, max_length=512, truncation=True, padding=True)\n",
        "        y = self.tokenizer(batch['response'], add_special_tokens=True, max_length=100, truncation=True, padding=True)\n",
        "        X = torch.tensor(X[\"input_ids\"])\n",
        "        y = torch.tensor(y['input_ids'])\n",
        "        X = X.to(device); y = y.to(device)\n",
        "        outputs = self.model(input_ids=X, decoder_input_ids=y, labels=y) \n",
        "        losses.append(outputs.loss.detach().item())\n",
        "        outputs.loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # break \n",
        "      logging.info(f'Epoch number: {epoch+1} Train Loss is equal: {sum(losses)/len(losses)}') \n",
        "      self.random_predict(test_dataloader, device, number_of_samples=10)\n",
        "      self.eval(eval_dataloader, epoch, device)\n",
        "      self.save(f\"/content/drive/MyDrive/models/autoencoder_{epoch}_{datetime.today().strftime('%Y-%m-%d')}.pt\")\n",
        "\n",
        "\n",
        "  def eval(self, dataloader, epoch, device):\n",
        "    self.model = self.model.to(device)\n",
        "    self.model.eval()\n",
        "    losses = []\n",
        "    for _, batch in enumerate(tqdm(dataloader, position=0, leave=True, desc=f\"Eval Epoch Number {epoch+1}\")):\n",
        "      with torch.no_grad():\n",
        "        X = self.tokenizer(batch['input'], add_special_tokens=True, max_length=512, truncation=True, padding='longest')\n",
        "        y = self.tokenizer(batch['response'], add_special_tokens=True, max_length=100, truncation=True, padding='longest')\n",
        "        X = torch.tensor(X[\"input_ids\"])\n",
        "        y = torch.tensor(y['input_ids'])\n",
        "        X = X.to(device); y = y.to(device)\n",
        "        outputs = self.model(input_ids=X, decoder_input_ids=y, labels=y) \n",
        "        \n",
        "        losses.append(outputs.loss.detach().item())\n",
        "        # break\n",
        "    logging.info(f'Epoch number: {epoch+1} Eval Loss is equal: {sum(losses)/len(losses)}')\n",
        "  \n",
        "  def save(self, save_path='./models/autoencoder.pt'):\n",
        "    logging.info(f'Saving model ...')\n",
        "    torch.save(self.model, save_path)\n",
        "\t\n",
        "  def load(self, save_path='./models/autoencoder.pt'):\n",
        "    logging.info(f'Loading model ...')\n",
        "    self.model = torch.load(save_path)\n",
        "\n",
        "  def random_predict(self, dataloader, device, number_of_samples=10):\n",
        "    counter=0\n",
        "    for sample in dataloader:\n",
        "      counter+=1\n",
        "      _input = self.tokenizer(sample['input'], add_special_tokens=True, max_length=512, padding=True)\n",
        "      _input = torch.tensor(_input['input_ids'])\n",
        "      _input = _input.to(device)\n",
        "      self.model = self.model.to(device)\n",
        "      decoder_start = torch.tensor(30526).to(device)\n",
        "      generated = self.model.generate(_input, decoder_start_token_id=torch.tensor(30526).to(device))\n",
        "      logging.info('Real: '+ sample['response'][0])\n",
        "      logging.info(tokenizer.convert_ids_to_tokens(generated[0]))\n",
        "      if counter>number_of_samples:\n",
        "        break \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T-Q7C9vFZ10"
      },
      "source": [
        "max_input, max_output = 0, 0\n",
        "for _, batch in enumerate(tqdm(train_dataloader, position=0, leave=True, desc=f\"Train Stats\")):\n",
        "  # self.model.zero_grad()\n",
        "  X = tokenizer(batch['input'], add_special_tokens=True, max_length=512, padding=True)\n",
        "  y = tokenizer(batch['response'], add_special_tokens=True, max_length=100, padding=True)\n",
        "  max_input\n",
        "  temp_input = max([len(item) for item in X[\"input_ids\"]])\n",
        "  temp_output = max([len(item) for item in y['input_ids']])\n",
        "  max_input = max(max_input, temp_input)\n",
        "  max_output = max(max_output, temp_output)\n",
        "  \n",
        "print(max_input, max_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWwDqSD8ZeB-"
      },
      "source": [
        "# \n",
        "\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "model.get_encoder().resize_token_embeddings(len(tokenizer))\n",
        "model.get_decoder().resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "optimizer = AdamW\n",
        "kw = {'lr':0.0005, 'weight_decay':0.1}\n",
        "tl = TrainingLoop(model, tokenizer, optimizer, False, **kw)\n",
        "# tl.load('/content/drive/MyDrive/models/autoencoder_0_2021-03-27.pt')\n",
        "data = read_data()\n",
        "data_samples = data_to_samples(data)\n",
        "bertified_data = bertified(data_samples)\n",
        "train, valid = train_test_split(bertified_data, test_size=0.15, random_state=99)\n",
        "test_data = read_data()\n",
        "test_data_samples = data_to_samples(test_data, True)\n",
        "test = bertified(test_data_samples)\n",
        "\n",
        "train_dataset = PersonaDataset_v1(train)\n",
        "valid_dataset = PersonaDataset_v1(valid)\n",
        "test_dataset = PersonaDataset_v1(test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=6, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "tl.train(train_dataloader, valid_dataloader, test_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# tl.save()\n",
        "# ##################################################\n",
        "# tl.load()\n",
        "# tl.random_predict(test_dataloader, device, number_of_samples=10)\n",
        "# ##################################################\n",
        "# tl.readable_predict(device, print_result=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijzZHyrhz9A0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}