{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_autoencoder_personachat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e559ca1f8e184c8f854b50603bc582c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65efa75de3f74b3aa13afe3cc46f9159",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5561ad8ae396417cbc6f98c4e293d125",
              "IPY_MODEL_f91a2f63e0b44cea9873fdaac5c6ea03"
            ]
          }
        },
        "65efa75de3f74b3aa13afe3cc46f9159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5561ad8ae396417cbc6f98c4e293d125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3a7ea3bfd393469ab19641a6638cb3af",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 111722,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 111722,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_779c57fa467049fdbfb953d087edd116"
          }
        },
        "f91a2f63e0b44cea9873fdaac5c6ea03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5d02af07bfe6447aba563e88663286c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 111722/111722 [08:31&lt;00:00, 218.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a11c55199e40413eb6dd2f345008815c"
          }
        },
        "3a7ea3bfd393469ab19641a6638cb3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "779c57fa467049fdbfb953d087edd116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d02af07bfe6447aba563e88663286c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a11c55199e40413eb6dd2f345008815c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16a15fe08d9a4566bea55bd562954452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5e1a28d4a0a54ff9b8775066b519da5a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10b6f0cbb42246eca71363393563fb5d",
              "IPY_MODEL_6ab8aa1f74644849841ccfbd1ee4a01d"
            ]
          }
        },
        "5e1a28d4a0a54ff9b8775066b519da5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10b6f0cbb42246eca71363393563fb5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd53b00ced2d40248872495073ac7551",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 19716,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19716,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f96154a77c645639168300de6bedd9e"
          }
        },
        "6ab8aa1f74644849841ccfbd1ee4a01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_27107a3e346c40c584acc77046f6681d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 19716/19716 [02:09&lt;00:00, 152.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f01a4623b04640d8990a836b3e3377f8"
          }
        },
        "cd53b00ced2d40248872495073ac7551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f96154a77c645639168300de6bedd9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27107a3e346c40c584acc77046f6681d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f01a4623b04640d8990a836b3e3377f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8499ec0a67b54b4886f66b1510b97f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d59a58d691dd42efa8512a5437881932",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0349bf513540461ba807c2494bfe4316",
              "IPY_MODEL_27f7098547b14a829454cf81e3502e89"
            ]
          }
        },
        "d59a58d691dd42efa8512a5437881932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0349bf513540461ba807c2494bfe4316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3079486ac2374bc482b20181d14a0ce6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7801,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7801,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_144c69c87b794e3fa99e50c3a7e7f9d5"
          }
        },
        "27f7098547b14a829454cf81e3502e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7da5a8ca91745b1a0a2353b6966f20d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7801/7801 [01:00&lt;00:00, 128.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06e51516caac42d99becf186f081baac"
          }
        },
        "3079486ac2374bc482b20181d14a0ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "144c69c87b794e3fa99e50c3a7e7f9d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7da5a8ca91745b1a0a2353b6966f20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06e51516caac42d99becf186f081baac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meti-94/TextGeneration/blob/main/bert_autoencoder_personachat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdM9KHPaRNmD"
      },
      "source": [
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar -xf  simple-examples.tgz\n",
        "!mkdir data\n",
        "!mv /content/simple-examples/data/ptb.train.txt data/\n",
        "!mv /content/simple-examples/data/ptb.valid.txt data/\n",
        "!mv /content/simple-examples/data/ptb.test.txt data/\n",
        "!rm -rf ./simple_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B71K5lC_Xnyg",
        "outputId": "b6563f5e-d78a-42ab-a35a-d7a8e894d030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "!wget https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\n",
        "!mkdir models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "--2021-04-20 14:04:40--  https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.48.254\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.48.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209850483 (200M) [application/json]\n",
            "Saving to: â€˜personachat_self_original.jsonâ€™\n",
            "\n",
            "personachat_self_or 100%[===================>] 200.13M  54.5MB/s    in 3.7s    \n",
            "\n",
            "2021-04-20 14:04:44 (53.9 MB/s) - â€˜personachat_self_original.jsonâ€™ saved [209850483/209850483]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iivSufj9QhvS",
        "outputId": "9c0a2d7b-b68d-4f5f-fa82-6dae47f9b0ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S5ZszffTUhl"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import EncoderDecoderModel, BertTokenizer, BertModel, BertConfig\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random \n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVUFk1dqSTbG"
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<persona>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>', '<persona>']}\n",
        "\n",
        "encoder_max_length=512\n",
        "decoder_max_length=128\n",
        "\n",
        "def read_data(data_json_path='/content/personachat_self_original.json'):\n",
        "  with open(data_json_path) as json_file:\n",
        "    data_dict = json.load(json_file)\n",
        "  return data_dict\n",
        "\n",
        "def data_to_samples(data_dict, test=False):\n",
        "  samples=[]\n",
        "  for dialogue in (data_dict['train'] if test==False else data_dict['valid']):\n",
        "    original_persona = dialogue['personality']\n",
        "    \n",
        "    for item in dialogue['utterances']:\n",
        "      original_persona = [original_persona[-1]] + original_persona[:-1]\n",
        "      history = item['history']\n",
        "      response = item['candidates'][-1]\n",
        "      samples.append({\n",
        "          'persona':original_persona,\n",
        "          'history':history,\n",
        "          'response':response\n",
        "      })\n",
        "  return samples\n",
        "  \n",
        "def bertified(samples):\n",
        "  bertified_data = []\n",
        "  for item in samples:\n",
        "    persona = ' <persona> '.join(item['persona'])\n",
        "    persona = '<bos> ' + persona\n",
        "    history = ''\n",
        "    speakers = [\" <speaker1> \", \" <speaker2> \"]\n",
        "    speaker = 0\n",
        "    for hst in item['history'][::-1]:\n",
        "      history = speakers[speaker] + hst + history\n",
        "      speaker = 1 - speaker\n",
        "    response = '<speaker2> ' + item['response'] + ' <eos>'\n",
        "    bertified_data.append({\n",
        "          'persona':persona.replace('  ', ' '),\n",
        "          'history':history.replace('  ', ' '),\n",
        "          'input': persona.replace('  ', ' ')+' '+history.replace('  ', ' '), \n",
        "          'response':response.replace('  ', ' ')\n",
        "      })\n",
        "  return bertified_data\n",
        "\n",
        "def bert_sample_tokenizer(bert_sample, tokenizer):\n",
        "  inputs = tokenizer(bert_sample['input'], add_special_tokens=True, max_length=encoder_max_length, truncation=True, padding=\"max_length\")\n",
        "  outputs = tokenizer(bert_sample['response'], add_special_tokens=True, max_length=decoder_max_length, truncation=True, padding=\"max_length\")\n",
        "  _item = {}\n",
        "  _item[\"input_ids\"] = inputs.input_ids\n",
        "  _item[\"attention_mask\"] = inputs.attention_mask\n",
        "  _item[\"decoder_input_ids\"] = outputs.input_ids\n",
        "  _item[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "  _item[\"labels\"] = outputs.input_ids.copy()\n",
        "  return _item "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRaoGJMZuIhb"
      },
      "source": [
        "|# print(type(test), type(test[0]), test[0])\n",
        "# print(type(test_data_samples), test_data_samples[0])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Hl8wbUs9O_"
      },
      "source": [
        "# encoder_max_length=512\n",
        "# decoder_max_length=128\n",
        "\n",
        "# def process_data_to_model_inputs(batch):\n",
        "#   # tokenize the inputs and labels\n",
        "#   inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "#   outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "#   batch[\"input_ids\"] = inputs.input_ids\n",
        "#   batch[\"attention_mask\"] = inputs.attention_mask\n",
        "#   batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "#   batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "#   batch[\"labels\"] = outputs.input_ids.copy()\n",
        "\n",
        "#   # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
        "#   # We have to make sure that the PAD token is ignored\n",
        "#   batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "\n",
        "#   return batch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrcvjwRn1zCl"
      },
      "source": [
        "class PersonaDataset_v3(Dataset):\n",
        "  '''\n",
        "      Convert Data to proper Tensor dataset\n",
        "  '''\n",
        "  def __init__(self, samples):\n",
        "    self.samples = samples\n",
        "    self.n_samples = len(self.samples)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # returns specific item\n",
        "    return self.samples[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF5aOQSztDrF"
      },
      "source": [
        "# X = self.tokenizer(batch['input'], add_special_tokens=True, max_length=512, truncation=True, padding=True)\n",
        "# y = self.tokenizer(batch['response'], add_special_tokens=True, max_length=100, truncation=True, padding=True)\n",
        "# X = torch.tensor(X[\"input_ids\"])\n",
        "# y = torch.tensor(y['input_ids'])\n",
        "# encoder_max_length=512\n",
        "# decoder_max_length=128\n",
        "# class PersonaDataset_v2(Dataset):\n",
        "#   '''\n",
        "#       Convert Data to proper Tensor dataset\n",
        "#   '''\n",
        "#   def __init__(self, samples):\n",
        "#     self.samples = samples\n",
        "#     self.n_samples = len(self.samples)\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     # returns specific item\n",
        "#     inputs = tokenizer(self.samples[index]['input'], add_special_tokens=True, max_length=encoder_max_length, truncation=True, padding=\"max_length\")\n",
        "#     outputs = tokenizer(self.samples[index]['response'], add_special_tokens=True, max_length=decoder_max_length, truncation=True, padding=\"max_length\")\n",
        "#     _item = {}\n",
        "#     _item[\"input_ids\"] = inputs.input_ids\n",
        "#     _item[\"attention_mask\"] = inputs.attention_mask\n",
        "#     _item[\"decoder_input_ids\"] = outputs.input_ids\n",
        "#     _item[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "#     _item[\"labels\"] = outputs.input_ids.copy()\n",
        "#     return _item\n",
        "#   def __len__(self):\n",
        "#     return self.n_samples\n",
        "    # returns dataset length"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iV8XBcrRTcQ"
      },
      "source": [
        "# class PersonaDataset_v1(Dataset):\n",
        "#   '''\n",
        "#       Convert Data to proper Tensor dataset\n",
        "#   '''\n",
        "#   def __init__(self, samples):\n",
        "#     self.samples = samples\n",
        "#     self.n_samples = len(self.samples)\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     # returns specific item\n",
        "#     return self.samples[index] \n",
        "#   def __len__(self):\n",
        "#     return self.n_samples\n",
        "#     # returns dataset length\n",
        "\n",
        "\n",
        "# class PTBDataset(Dataset):\n",
        "#   '''\n",
        "#       Convert Data to proper Tensor dataset\n",
        "#   '''\n",
        "#   def __init__(self, path):\n",
        "#     self.texts = []\n",
        "#     with open(path, 'r') as fin:\n",
        "#       for line in fin:\n",
        "#         self.texts.append(line.strip())\n",
        "#     self.n_samples = len(self.texts)\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     # returns specific item\n",
        "#     return self.texts[index] \n",
        "#   def __len__(self):\n",
        "#     return self.n_samples\n",
        "#     # returns dataset length\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUUvf3MRpWwH",
        "outputId": "e719110d-6173-42dc-baaa-668e67e0407a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721,
          "referenced_widgets": [
            "e559ca1f8e184c8f854b50603bc582c4",
            "65efa75de3f74b3aa13afe3cc46f9159",
            "5561ad8ae396417cbc6f98c4e293d125",
            "f91a2f63e0b44cea9873fdaac5c6ea03",
            "3a7ea3bfd393469ab19641a6638cb3af",
            "779c57fa467049fdbfb953d087edd116",
            "5d02af07bfe6447aba563e88663286c9",
            "a11c55199e40413eb6dd2f345008815c",
            "16a15fe08d9a4566bea55bd562954452",
            "5e1a28d4a0a54ff9b8775066b519da5a",
            "10b6f0cbb42246eca71363393563fb5d",
            "6ab8aa1f74644849841ccfbd1ee4a01d",
            "cd53b00ced2d40248872495073ac7551",
            "8f96154a77c645639168300de6bedd9e",
            "27107a3e346c40c584acc77046f6681d",
            "f01a4623b04640d8990a836b3e3377f8",
            "8499ec0a67b54b4886f66b1510b97f80",
            "d59a58d691dd42efa8512a5437881932",
            "0349bf513540461ba807c2494bfe4316",
            "27f7098547b14a829454cf81e3502e89",
            "3079486ac2374bc482b20181d14a0ce6",
            "144c69c87b794e3fa99e50c3a7e7f9d5",
            "f7da5a8ca91745b1a0a2353b6966f20d",
            "06e51516caac42d99becf186f081baac"
          ]
        }
      },
      "source": [
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"prajjwal1/bert-tiny\", \"prajjwal1/bert-tiny\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "model.get_encoder().resize_token_embeddings(len(tokenizer))\n",
        "model.get_decoder().resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "data = read_data()\n",
        "data_samples = data_to_samples(data)\n",
        "bertified_data = bertified(data_samples)\n",
        "train, valid = train_test_split(bertified_data, test_size=0.15, random_state=99)\n",
        "test_data = read_data()\n",
        "test_data_samples = data_to_samples(test_data, True)\n",
        "test = bertified(test_data_samples)\n",
        "\n",
        "one_line_tokenizer = lambda bert_sample:bert_sample_tokenizer(bert_sample, tokenizer)\n",
        "\n",
        "# train_dataset = PersonaDataset_v3(list(zip(*map(one_line_tokenizer, tqdm(train)))))\n",
        "valid_dataset = PersonaDataset_v3(list(map(one_line_tokenizer, tqdm(train))))\n",
        "valid_dataset = PersonaDataset_v3(list(map(one_line_tokenizer, tqdm(valid))))\n",
        "test_dataset = PersonaDataset_v3(list(map(one_line_tokenizer, tqdm(test))))\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /prajjwal1/bert-tiny/resolve/main/tokenizer.json HTTP/1.1\" 404 0\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e559ca1f8e184c8f854b50603bc582c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=111722.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16a15fe08d9a4566bea55bd562954452",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=19716.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8499ec0a67b54b4886f66b1510b97f80",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=7801.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ZhEGdl9JX5",
        "outputId": "4c1c70a6-5bcc-44d5-9113-54160811033e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "valid_dataset[0].keys()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXfk_c5-qhAi"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nxdqTEBzorK"
      },
      "source": [
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "# model.config.max_length = 142\n",
        "# model.config.min_length = 56\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.early_stopping = True\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGldq9Avo7Ki",
        "outputId": "e34aa9ab-7bfe-4f62-9ab9-56c361c8b80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "batch_size = 128\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True, \n",
        "    output_dir=\"./\",\n",
        "    logging_steps=2,\n",
        "    save_steps=10,\n",
        "    eval_steps=4,\n",
        "    # logging_steps=1000,\n",
        "    # save_steps=500,\n",
        "    # eval_steps=7500,\n",
        "    # warmup_steps=2000,\n",
        "    # save_total_limit=3,\n",
        ")\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-ce29a907cb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# on the whole batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# on the whole batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj4jILQtXmnt"
      },
      "source": [
        "class TrainingLoop:\n",
        "  '''\n",
        "  Everything related to model training\n",
        "  '''\n",
        "  def __init__( self, model, tokenizer, optimizer, freezeemb=True, \n",
        "                epochs=6, save_path='./models/', **kw):\n",
        "    self.model = model\n",
        "    params = []\n",
        "    for paramname, param in self.model.named_parameters():\n",
        "      if paramname.startswith(\"bert.embeddings.word_embeddings\"):\n",
        "        if not freezeemb:\n",
        "          params.append(param)\n",
        "      else:\n",
        "        params.append(param)\n",
        "    self.optimizer = optimizer(params, **kw)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.epochs = epochs\n",
        "    self.save_path = save_path\n",
        "    self.predicts = None\n",
        "\n",
        "\n",
        "  def train(self, dataloader, eval_dataloader, test_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = self.model.to(device)\n",
        "    for epoch in range(self.epochs):\n",
        "      self.model.train()\n",
        "      losses = []\n",
        "\n",
        "      for _, batch in enumerate(tqdm(dataloader, position=0, leave=True, desc=f\"Train Epoch Number {epoch+1}\")):\n",
        "        self.model.zero_grad()\n",
        "        X = self.tokenizer(batch['input'], add_special_tokens=True, max_length=512, truncation=True, padding=True)\n",
        "        y = self.tokenizer(batch['response'], add_special_tokens=True, max_length=100, truncation=True, padding=True)\n",
        "        X = torch.tensor(X[\"input_ids\"])\n",
        "        y = torch.tensor(y['input_ids'])\n",
        "        X = X.to(device); y = y.to(device)\n",
        "        outputs = self.model(input_ids=X, decoder_input_ids=y, labels=y) \n",
        "        losses.append(outputs.loss.detach().item())\n",
        "        outputs.loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # break \n",
        "      logging.info(f'Epoch number: {epoch+1} Train Loss is equal: {sum(losses)/len(losses)}') \n",
        "      self.random_predict(test_dataloader, device, number_of_samples=10)\n",
        "      self.eval(eval_dataloader, epoch, device)\n",
        "      self.save(f\"/content/drive/MyDrive/models/autoencoder_{epoch}_{datetime.today().strftime('%Y-%m-%d')}.pt\")\n",
        "\n",
        "\n",
        "  def eval(self, dataloader, epoch, device):\n",
        "    self.model = self.model.to(device)\n",
        "    self.model.eval()\n",
        "    losses = []\n",
        "    for _, batch in enumerate(tqdm(dataloader, position=0, leave=True, desc=f\"Eval Epoch Number {epoch+1}\")):\n",
        "      with torch.no_grad():\n",
        "        X = self.tokenizer(batch['input'], add_special_tokens=True, max_length=512, truncation=True, padding='longest')\n",
        "        y = self.tokenizer(batch['response'], add_special_tokens=True, max_length=100, truncation=True, padding='longest')\n",
        "        X = torch.tensor(X[\"input_ids\"])\n",
        "        y = torch.tensor(y['input_ids'])\n",
        "        X = X.to(device); y = y.to(device)\n",
        "        outputs = self.model(input_ids=X, decoder_input_ids=y, labels=y) \n",
        "        \n",
        "        losses.append(outputs.loss.detach().item())\n",
        "        # break\n",
        "    logging.info(f'Epoch number: {epoch+1} Eval Loss is equal: {sum(losses)/len(losses)}')\n",
        "  \n",
        "  def save(self, save_path='./models/autoencoder.pt'):\n",
        "    logging.info(f'Saving model ...')\n",
        "    torch.save(self.model, save_path)\n",
        "\t\n",
        "  def load(self, save_path='./models/autoencoder.pt'):\n",
        "    logging.info(f'Loading model ...')\n",
        "    self.model = torch.load(save_path)\n",
        "\n",
        "  def random_predict(self, dataloader, device, number_of_samples=10):\n",
        "    counter=0\n",
        "    for sample in dataloader:\n",
        "      counter+=1\n",
        "      _input = self.tokenizer(sample['input'], add_special_tokens=True, max_length=512, padding=True)\n",
        "      _input = torch.tensor(_input['input_ids'])\n",
        "      _input = _input.to(device)\n",
        "      self.model = self.model.to(device)\n",
        "      decoder_start = torch.tensor(30526).to(device)\n",
        "      generated = self.model.generate(_input, decoder_start_token_id=torch.tensor(30526).to(device))\n",
        "      logging.info('Real: '+ sample['response'][0])\n",
        "      logging.info(tokenizer.convert_ids_to_tokens(generated[0]))\n",
        "      if counter>number_of_samples:\n",
        "        break \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWwDqSD8ZeB-"
      },
      "source": [
        "# \n",
        "\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "model.get_encoder().resize_token_embeddings(len(tokenizer))\n",
        "model.get_decoder().resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "optimizer = AdamW\n",
        "kw = {'lr':0.0005, 'weight_decay':0.1}\n",
        "tl = TrainingLoop(model, tokenizer, optimizer, False, **kw)\n",
        "# tl.load('/content/drive/MyDrive/models/autoencoder_0_2021-03-27.pt')\n",
        "data = read_data()\n",
        "data_samples = data_to_samples(data)\n",
        "bertified_data = bertified(data_samples)\n",
        "train, valid = train_test_split(bertified_data, test_size=0.15, random_state=99)\n",
        "test_data = read_data()\n",
        "test_data_samples = data_to_samples(test_data, True)\n",
        "test = bertified(test_data_samples)\n",
        "\n",
        "train_dataset = PersonaDataset_v1(train)\n",
        "valid_dataset = PersonaDataset_v1(valid)\n",
        "test_dataset = PersonaDataset_v1(test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=6, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "tl.train(train_dataloader, valid_dataloader, test_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# tl.save()\n",
        "# ##################################################\n",
        "# tl.load()\n",
        "# tl.random_predict(test_dataloader, device, number_of_samples=10)\n",
        "# ##################################################\n",
        "# tl.readable_predict(device, print_result=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijzZHyrhz9A0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}